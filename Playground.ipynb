{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from operator import itemgetter\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import (\n",
    "                                    Dense,\n",
    "                                    Conv2D, \n",
    "                                    BatchNormalization, \n",
    "                                    ReLU, \n",
    "                                    Add,\n",
    "                                    Input,\n",
    "                                    MaxPooling2D,\n",
    "                                    UpSampling2D,\n",
    "                                    )\n",
    "from keras.models import Model, load_model\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from math import exp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de Datos/Preprocesamiento del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_MIN_OF_IMGS_FOR_TRAINING = 0\n",
    "NUMBER_MAX_OF_IMGS_FOR_TRAINING = 11400\n",
    "########################################################################################################################################################\n",
    "def generate_dataset_obj(obj):\n",
    "    if type(obj) == np.ndarray:\n",
    "        dim = obj.shape[0]\n",
    "        if dim == 1:\n",
    "            ret = generate_dataset_obj(obj[0])             \n",
    "        else:\n",
    "            ret = []\n",
    "            for i in range(dim):\n",
    "                ret.append(generate_dataset_obj(obj[i]))                \n",
    "\n",
    "    elif type(obj) == scipy.io.matlab.mio5_params.mat_struct:\n",
    "        ret = {}\n",
    "        for field_name in obj._fieldnames:            \n",
    "            field = generate_dataset_obj(obj.__dict__[field_name])\n",
    "            if field_name in must_be_list_fields:\n",
    "                field = [field]\n",
    "                ret[field_name] = field\n",
    "\n",
    "    else:\n",
    "        ret = obj\n",
    "\n",
    "    return ret\n",
    "########################################################################################################################################################\n",
    "def generate_dataset_obj(obj):\n",
    "    if type(obj) == np.ndarray:\n",
    "        dim = obj.shape[0]\n",
    "        if dim == 1:\n",
    "            ret = generate_dataset_obj(obj[0])             \n",
    "        else:\n",
    "            ret = []\n",
    "            for i in range(dim):\n",
    "                ret.append(generate_dataset_obj(obj[i]))                \n",
    "\n",
    "    elif type(obj) == scipy.io.matlab.mio5_params.mat_struct:\n",
    "        ret = {}\n",
    "        for field_name in obj._fieldnames:            \n",
    "            field = generate_dataset_obj(obj.__dict__[field_name])\n",
    "            if field_name in must_be_list_fields:\n",
    "                field = [field]\n",
    "                ret[field_name] = field\n",
    "\n",
    "    else:\n",
    "        ret = obj\n",
    "\n",
    "    return ret\n",
    "\n",
    "########################################################################################################################################################\n",
    "def print_dataset_obj(obj, depth = 0, maxIterInArray = 20):\n",
    "    prefix = \"  \"*depth\n",
    "    if type(obj) == dict:\n",
    "        for key in obj.keys():\n",
    "            print(\"{}{}\".format(prefix, key))\n",
    "            print_dataset_obj(obj[key], depth + 1)\n",
    "    elif type(obj) == list:\n",
    "        for i, value in enumerate(obj):\n",
    "            if i >= maxIterInArray:\n",
    "                break\n",
    "            print(\"{}{}\".format(prefix, i))\n",
    "            print_dataset_obj(value, depth + 1)\n",
    "    else:\n",
    "        print(\"{}{}\".format(prefix, obj))\n",
    "########################################################################################################################################################\n",
    "def return_image_joints(name,data):\n",
    "    for item in data: # guardar coordenadas de los joints\n",
    "        if item[0] == name:\n",
    "            #print(item[1]) \n",
    "            return item[1]\n",
    "########################################################################################################################################################\n",
    "rightconnections = [\n",
    "                    (0,1),(1,2),(3,4),(4,5),(2,6),\n",
    "                    (3,6),(6,7),(7,8),(8,9),(10,11),\n",
    "                    (11,12),(12,7),(13,7),(13,14),(14,15)\n",
    "                   ]\n",
    "size_img_x = 256\n",
    "size_img_y = 256\n",
    "def draw_img_joints(file_name, data, resize = False ):    \n",
    "    # Load image\n",
    "    #img = cv2.imread(Path_To_Single_Person_Images + \"/\" + file_name,1)  \n",
    "    img = image.load_img(Path_To_Single_Person_Images + \"/\" + file_name)\n",
    "    img = image.img_to_array(img) \n",
    "    img = img/255\n",
    "    if resize:\n",
    "        img = np.float32(tf.image.resize(img,(size_img_x, size_img_y)))  \n",
    "    pts = return_image_joints(file_name, data)        \n",
    "    #plt.imshow(img)  \n",
    "    X = [x[0] for x in pts]\n",
    "    Y = [y[1] for y in pts]\n",
    "    X = [int(x) for x in X]\n",
    "    Y = [int(y) for y in Y]\n",
    "    \n",
    "    for i in range(16):\n",
    "        for j in range(16):\n",
    "            if (i,j) in rightconnections:\n",
    "                if X[i]>0 and X[j]>0 and Y[i]>0 and Y[j]>0:\n",
    "                    img = cv2.line(img,(X[i],Y[i]),(X[j],Y[j]),(1,0,0),5)\n",
    "                    plt.scatter(X[i], Y[i], marker=\"o\", color=\"red\", s=20)\n",
    "                    plt.scatter(X[j], Y[j], marker=\"o\", color=\"red\", s=20)\n",
    "                    \n",
    "    plt.imshow(img)\n",
    "########################################################################################################################################################\n",
    "def crop_resize(imagen, joints, scale, margin = 0.2):\n",
    "    # CROP PART\n",
    "    xmin = 9999\n",
    "    ymin = 9999\n",
    "    xmax = -1\n",
    "    ymax = -1\n",
    "    body_height_margin = scale * 200 * margin\n",
    "    img_height = imagen.shape[0]\n",
    "    img_width = imagen.shape[1]\n",
    "    for item in joints:\n",
    "        if item[0] >= 0 and item[0] < xmin : xmin = item[0]\n",
    "        if item[1] >= 0 and item[1] < ymin : ymin = item[1] \n",
    "        if item[0] >= 0 and item[0] > xmax : xmax = item[0]\n",
    "        if item[1] >= 0 and item[1] > ymax : ymax = item[1]    \n",
    "    xmin = int(xmin - body_height_margin)\n",
    "    ymin = int(ymin - body_height_margin)\n",
    "    xmax = int(xmax + body_height_margin)\n",
    "    ymax = int(ymax + body_height_margin)\n",
    "    if xmin < 0: xmin = 0\n",
    "    if ymin < 0: ymin = 0\n",
    "    if xmax > img_width: xmax = img_width\n",
    "    if ymax > img_height: ymax = img_height\n",
    "    imagen = imagen[ymin:ymax, xmin:xmax, :]\n",
    "    #print(\"xmin:\",xmin,\", ymin: \",ymin,\", xmax: \",xmax,\", ymax: \",ymax)\n",
    "    #RESIZE PART\n",
    "    img_new_height = imagen.shape[0]\n",
    "    img_new_width = imagen.shape[1]\n",
    "    scala_x = img_new_width / size_img_x\n",
    "    scala_y = img_new_height / size_img_y\n",
    "    for i in range(16): # escala los puntos clave\n",
    "        joints[i] = np.array([(joints[i][0] - xmin) / scala_x, (joints[i][1] - ymin) / scala_y]) \n",
    "        #print(\"x:\",joints[i][0],\", y: \",joints[i][1])\n",
    "    imagen = tf.image.resize(imagen,(size_img_x, size_img_y))       \n",
    "    imagen = imagen/255\n",
    "    \n",
    "    return imagen\n",
    "########################################################################################################################################################    \n",
    "def load_image(train_data, a, b):\n",
    "    train = np.asarray(train_data[a:b])\n",
    "    train_image = np.zeros((b-a,size_img_x,size_img_y,3))\n",
    "    #print(train[2])\n",
    "    for i in tqdm(range(a,b)):\n",
    "        name_img = train[i][0]\n",
    "        img = image.load_img(Path_To_Single_Person_Images + '/' + name_img)\n",
    "        img = image.img_to_array(img) \n",
    "        #crop and resize\n",
    "        train_image[i] = crop_resize(img, train[i][1], train[i][2])\n",
    "    return train_image, train\n",
    "########################################################################################################################################################\n",
    "def MakeHeatmap(x, y, width, height, show = False):\n",
    "    # Probability as a function of distance from the center derived\n",
    "    # from a gaussian distribution with mean = 0 and stdv = 1\n",
    "    scaledGaussian = lambda x : exp(-(1/2)*(x**2))\n",
    "\n",
    "    imgSize = (height, width)\n",
    "    center_x = int(round(x)) #redondeamos\n",
    "    center_y = int(round(y))\n",
    "\n",
    "    isotropicGrayscaleImage = np.zeros((imgSize[0],imgSize[1]),np.uint8)\n",
    "    \n",
    "    if center_x > 0 and center_y > 0 :\n",
    "        for i in np.unique(np.clip(np.array(range(center_y-3,center_y+4,1)),0,height-1)): #solo queremos calcular un parche de 7x7, ademas debemos evitar valores fuera de la matriz, finalmente quitamos valores repetidos\n",
    "            for j in np.unique(np.clip(np.array(range(center_x-3,center_x+4,1)),0,width-1)):\n",
    "\n",
    "                # find euclidian distance from center of image (x,y) \n",
    "                # and scale it to range of 0 to 2.5 as scaled Gaussian\n",
    "                # returns highest probability for x=0 and approximately\n",
    "                # zero probability for x > 2.5\n",
    "\n",
    "                distanceFromCenter = np.linalg.norm(np.array([i-center_y,j-center_x]))\n",
    "                #distanceFromCenter = 18*distanceFromCenter/(imgSize/2)\n",
    "                scaledGaussianProb = scaledGaussian(distanceFromCenter)                \n",
    "                isotropicGrayscaleImage[i,j] = np.clip(scaledGaussianProb*255,0,255) \n",
    "                \n",
    "        return isotropicGrayscaleImage\n",
    "    else: \n",
    "        return isotropicGrayscaleImage\n",
    "########################################################################################################################################################    \n",
    "def Joints_heatmaps(lista_de_joints, heatmap_size_x, heatmap_size_y, num_heatmaps = 16, show = False):\n",
    "    heatmaps = np.zeros((16,64,64))\n",
    "    for i in range(num_heatmaps):\n",
    "        x, y = lista_de_joints[i] \n",
    "        x = x / 4 # entre 4 por que el array es de 256x256\n",
    "        y = y / 4 # entre 4 por que el array es de 256x256\n",
    "        heatmaps[i] = MakeHeatmap(x, y, heatmap_size_x, heatmap_size_y)\n",
    "    if show:\n",
    "        plotImages(heatmaps, num_heatmaps)\n",
    "    return heatmaps\n",
    "########################################################################################################################################################        \n",
    "def plotImages(images_arr, num_images):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(60,60))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "########################################################################################################################################################    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar datos, Generación de heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divimos la data para ingresarla al modelo,\n",
    "if os.path.isfile('X_train.npy') and os.path.isfile('X_test.npy') and os.path.isfile('y_train.npy') and os.path.isfile('y_test.npy'):    \n",
    "    #Primero comprobamos si existen ya datos para usar en el modelo, si hay los mapeamos, no se cargan a la ram, se leen desde disco \n",
    "    X_train = np.load('X_train.npy', mmap_mode='r')\n",
    "    X_test = np.load('X_test.npy', mmap_mode='r')\n",
    "    y_train = np.load('y_train.npy', mmap_mode='r')\n",
    "    y_test = np.load('y_test.npy', mmap_mode='r')  \n",
    "else:      \n",
    "    # si no existen entonces iniciamos el preprocesado:   \n",
    "    ########################################################################################################################################################\n",
    "    if os.path.isfile('lista_de_imagenes.npy') and os.path.isfile('lista_de_heatmaps.npy'):    \n",
    "        #Ahora cargamos las imagenes y sus heatmaps\n",
    "        lista_de_heatmaps = np.load('lista_de_heatmaps.npy', mmap_mode='r')  \n",
    "        lista_de_imagenes = np.load('lista_de_imagenes.npy', mmap_mode='r')    \n",
    "    else: \n",
    "        ########################################################################################################################################################\n",
    "        #darle formato de diccionario\n",
    "        matph = './mpii.mat'\n",
    "        decoded1 = loadmat(matph, struct_as_record=False)[\"RELEASE\"]\n",
    "        must_be_list_fields = [\"annolist\",\"image\",\"name\", \"annorect\", \"scale\", \"x\", \"y\", \"annopoints\", \"point\", \"id\"]\n",
    "        # Convert to dict\n",
    "        dataset_obj = generate_dataset_obj(decoded1)\n",
    "        # Print it out\n",
    "        #print_dataset_obj(dataset_obj)\n",
    "        len(dataset_obj['annolist'][0])\n",
    "        #solo queremos la información en 'annolist'\n",
    "        dataset = dataset_obj['annolist'][0]\n",
    "        ########################################################################################################################################################\n",
    "        #guardamos solo informacion de las imagenes que tienen solo una persona\n",
    "        train_data = []\n",
    "        for i in range(len(dataset)):\n",
    "            if \"annopoints\" in dataset[i]['annorect'][0]:     \n",
    "                name = dataset[i]['image'][0]['name'][0]\n",
    "                scale = dataset[i]['annorect'][0]['scale'][0]\n",
    "                tupla = np.full((16, 3), -1) #creo un prototipo de array de joints lleno de -1 si la articulación es visible se reemplaza el -1\n",
    "                for j in range(len(dataset[i]['annorect'][0]['annopoints'][0]['point'][0])):     #ordena los puntos de articulaciones del id = 0 al id = 15       \n",
    "                    try:\n",
    "                        x = dataset[i]['annorect'][0]['annopoints'][0]['point'][0][j]['x'][0]\n",
    "                    except:\n",
    "                        x = -1\n",
    "                    try:\n",
    "                        y = dataset[i]['annorect'][0]['annopoints'][0]['point'][0][j]['y'][0]\n",
    "                    except:\n",
    "                        y = -1\n",
    "                    try:\n",
    "                        joint_id = dataset[i]['annorect'][0]['annopoints'][0]['point'][0][j]['id'][0]\n",
    "                    except:\n",
    "                        joint_id = -1          \n",
    "                    tupla[joint_id] = np.array([x,y,joint_id]) # esto lo ordena\n",
    "                #tupla = sorted(tupla, key = itemgetter(2)) \n",
    "                #tupla = tupla[np.argsort(tupla[:, 2])] \n",
    "                tupla = np.delete(tupla, 2, 1)  #quita id de las tuplas,                    \n",
    "                #pasa de tupla a array\n",
    "                tupla = np.asarray(tupla)        \n",
    "                train_data.append((name,tupla,scale))\n",
    "        #Creamos un array para guardar los nombres        \n",
    "        names = []\n",
    "        for item in train_data:#guardar nombres de las imagenes que voy a usar en \"name\"\n",
    "            names.append(item[0])\n",
    "        ########################################################################################################################################################\n",
    "        #Crear Carpeta para guardar imagenes del dataset\n",
    "        Path_To_Raw_Images = 'DataSet/mpii_human_pose_v1_images'\n",
    "        Path_To_Single_Person_Images = 'DataSet/mpii_human_pose_v1_images/SinglePersonImagesWithData'\n",
    "        os.chdir(Path_To_Raw_Images)\n",
    "\n",
    "        if os.path.isdir('SinglePersonImagesWithData') is False:\n",
    "            os.makedirs('SinglePersonImagesWithData')\n",
    "            for images in names:\n",
    "                shutil.move(images, 'SinglePersonImagesWithData')\n",
    "\n",
    "        os.chdir('../../')\n",
    "\n",
    "        #Demostración dibujar joints en imagenes con data\n",
    "        #draw_img_joints('060111501.jpg',train_data)\n",
    "        ########################################################################################################################################################\n",
    "        #Ahora cargamos las imagenes\n",
    "        lista_de_imagenes, lista_de_joints = load_image(train_data,NUMBER_MIN_OF_IMGS_FOR_TRAINING,NUMBER_MAX_OF_IMGS_FOR_TRAINING)\n",
    "        np.save('lista_de_imagenes', lista_de_imagenes)\n",
    "        ########################################################################################################################################################\n",
    "        #Ahora creamos los heatmaps para lista_de_imagenes:\n",
    "        heatmap_size_x = 64\n",
    "        heatmap_size_y = 64 \n",
    "        #dibujo_test_heatmaps = Joints_heatmaps(lista_de_joints[0][1], heatmap_size_x, heatmap_size_y, show = False)\n",
    "        #creamos los heatmaps de nuestra data\n",
    "        lista_de_heatmaps = np.zeros((NUMBER_MAX_OF_IMGS_FOR_TRAINING - NUMBER_MIN_OF_IMGS_FOR_TRAINING,heatmap_size_x,heatmap_size_y,16))\n",
    "        for i in tqdm(range(lista_de_joints.shape[0])):\n",
    "            joints = return_image_joints(lista_de_joints[i][0], lista_de_joints)\n",
    "            lista_de_heatmaps[i] = np.moveaxis(Joints_heatmaps(joints, heatmap_size_x, heatmap_size_y), 0, -1) # change shape from 16x64x64 to 64x64x16\n",
    "        #guardamos el array    \n",
    "        np.save('lista_de_heatmaps', lista_de_heatmaps)\n",
    "        #liberamos memoria cargando los archivos desde disco\n",
    "        lista_de_heatmaps = np.load('lista_de_heatmaps.npy', mmap_mode='r')  \n",
    "        lista_de_imagenes = np.load('lista_de_imagenes.npy', mmap_mode='r') \n",
    "        #plotImages(lista_de_heatmaps[67], 16)\n",
    "    ######################################################################################################################################################## \n",
    "    X_train, X_test, y_train, y_test = train_test_split(lista_de_imagenes, lista_de_heatmaps, random_state=7, test_size=0.2)\n",
    "    #guardamos en disco y liberamos ram\n",
    "    np.save('X_train', X_train)\n",
    "    X_train = np.load('X_train.npy', mmap_mode='r')\n",
    "    np.save('X_test', X_test)\n",
    "    X_test = np.load('X_test.npy', mmap_mode='r')\n",
    "    np.save('y_train', y_train)\n",
    "    y_train = np.load('y_train.npy', mmap_mode='r')\n",
    "    np.save('y_test', y_test)\n",
    "    y_test = np.load('y_test.npy', mmap_mode='r')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'###test part\\nimgidtest = 9\\nprint(\"image name: \",lista_de_joints[imgidtest][0])\\nprint(\"joints locations: \\n\",lista_de_joints[imgidtest][1])\\nprint(\"scale: \",lista_de_joints[imgidtest][2])\\nplt.imshow(lista_de_imagenes[imgidtest])\\nplotImages(np.moveaxis(lista_de_heatmaps[imgidtest], -1, 0),16)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"###test part\n",
    "imgidtest = 9\n",
    "print(\"image name: \",lista_de_joints[imgidtest][0])\n",
    "print(\"joints locations: \\n\",lista_de_joints[imgidtest][1])\n",
    "print(\"scale: \",lista_de_joints[imgidtest][2])\n",
    "plt.imshow(lista_de_imagenes[imgidtest])\n",
    "plotImages(np.moveaxis(lista_de_heatmaps[imgidtest], -1, 0),16)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D(inputs, \n",
    "           filters, \n",
    "           kernel_size = 1,\n",
    "           strides = 1,\n",
    "           padding = 'same',\n",
    "           kernel_initializer = 'he_normal',\n",
    "           activation = True,\n",
    "           batch_normalization = True,\n",
    "           name = \"conv\"):\n",
    "    \n",
    "    x = Conv2D(filters, kernel_size, strides=strides, padding=padding,\n",
    "            use_bias=False, kernel_initializer=kernel_initializer)(inputs)\n",
    "    if batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = ReLU()(x)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arquitectura ResNetV2 de cuello de botella\n",
    "def ResNetV2(inputs, filters, strides = 1, lift_channels = False, name = 'bloque'):\n",
    "    \n",
    "    res = inputs\n",
    "    #incrementa el número de canales si es necesario\n",
    "    if lift_channels:\n",
    "        res = conv2D(\n",
    "            inputs,\n",
    "            filters,\n",
    "            activation = False,\n",
    "            batch_normalization = False)\n",
    "    \n",
    "    x = BatchNormalization()(inputs)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    #conv de tamaño 1x1\n",
    "    x = conv2D(\n",
    "        x,\n",
    "        filters = filters/2)\n",
    "    \n",
    "    #conv de tamaño 3x3\n",
    "    x = conv2D(\n",
    "        x,\n",
    "        filters = filters/2,\n",
    "        kernel_size = 3)\n",
    "    \n",
    "    #conv de tamaño 1x1\n",
    "    x = conv2D(\n",
    "        x,\n",
    "        filters = filters,\n",
    "        activation = False,\n",
    "        batch_normalization = False)\n",
    "    \n",
    "    x = Add()([res,x])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassUnit(inputs, depth, filters, resnet_per_block, name = 'hourglass_'):    \n",
    "    \n",
    "    #Capas \"superiores\"\n",
    "    up_1 = ResNetV2(inputs,filters)\n",
    "    \n",
    "    for i in range(resnet_per_block):\n",
    "        up_1 = ResNetV2(up_1, filters)\n",
    "    \n",
    "    #Capas \"inferiores\"\n",
    "    #Reducir resolución\n",
    "    low_1 = MaxPooling2D(pool_size = 2, strides = 2)(inputs)\n",
    "    \n",
    "    for i in range(resnet_per_block):\n",
    "        low_1 = ResNetV2(low_1, filters)\n",
    "    \n",
    "    low_2 = low_1\n",
    "    if depth > 1 : \n",
    "        low_2 = HourglassUnit(low_1, depth-1, filters, resnet_per_block)\n",
    "    else:\n",
    "        low_2 = ResNetV2(low_2, filters)\n",
    "    \n",
    "    low_3 = low_2\n",
    "    \n",
    "    for i in range(resnet_per_block):\n",
    "        low_3 = ResNetV2(low_3, filters)\n",
    "    \n",
    "    #Aumentar resolución\n",
    "    up_2 = UpSampling2D()(low_3)\n",
    "    \n",
    "    return Add()([up_1,up_2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassNetwork(input_shape = (256,256,3), stacks = 8, resnet_per_block = 3, heatmaps = 16):\n",
    "    \n",
    "    inputs = Input(shape = input_shape)\n",
    "    \n",
    "    #la data llega en formato 256x256x3, la pasamos a 64x64x256\n",
    "    #preprocessing\n",
    "    #amplia canales a 64\n",
    "    x = conv2D(\n",
    "        inputs,\n",
    "        filters = 64,\n",
    "        kernel_size = 7,\n",
    "        strides = 2)\n",
    "    #amplia canales de 64 a 128 \n",
    "    x = ResNetV2(x, filters = 128, lift_channels = True)\n",
    "    x = MaxPooling2D(pool_size = 2, strides = 2)(x)\n",
    "    x = ResNetV2(x, filters = 128)\n",
    "    #amplia canales de 64 a 128\n",
    "    x = ResNetV2(x, filters = 256, lift_channels = True)\n",
    "    skip = x\n",
    "    y_heatmaps = []\n",
    "    \n",
    "    for i in range(stacks):\n",
    "        x = HourglassUnit(skip, depth = 4, filters = 256, resnet_per_block = resnet_per_block)\n",
    "        \n",
    "        x = ResNetV2(x, filters = 256)\n",
    "        \n",
    "        #prediccion de 256 canales \n",
    "        x = conv2D(x, filters = 256)\n",
    "        \n",
    "        #prediccion temporal de heatmaps\n",
    "        y = conv2D(x, filters = heatmaps)\n",
    "        #agregamos el resultado temportal al array de resultados para la supervision intermedia\n",
    "        y_heatmaps.append(y)\n",
    "        \n",
    "        #ahora regresamos el tensor y al orden de 256 canales si es que no es el ultimo output\n",
    "        if i < stacks - 1:\n",
    "            y_recovery1 = conv2D(x, filters = 256, activation = False, batch_normalization = False)\n",
    "            y_recovery2 = conv2D(y, filters = 256, activation = False, batch_normalization = False)\n",
    "            skip = Add()([skip, y_recovery1, y_recovery2])\n",
    "    #print(y_heatmaps)\n",
    "    return Model(inputs = inputs, outputs = y_heatmaps, name = 'HourglassNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# tasa de decrecimiento del learning rate por numero de epoch\n",
    "def lr_schedule(epoch): #tome esto de resnet.ipynb\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-4\n",
    "    if epoch > 180:\n",
    "        lr *= 1e-1\n",
    "    elif epoch > 150:\n",
    "        lr *= 1e-1\n",
    "    elif epoch > 100:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 50:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr \n",
    "############################################################################\n",
    "# custom loss mean_squared_error function, sirve para magnificar la presencia de todos los pixeles > que 0 en los mapas de calor\n",
    "def custom_loss_mse_function(y_true, y_pred):\n",
    "    weights = tf.cast(y_true > 0, dtype=tf.float32) * 81 + 1\n",
    "    return tf.reduce_mean(tf.math.square(y_true - y_pred) * weights)\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicio el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('single_person_modelvFULLDATA.h5'):\n",
    "    # verificamos si hay algun punto de guardado del modelo\n",
    "    model = load_model('single_person_modelvFULLDATA.h5', custom_objects={'custom_loss_mse_function': custom_loss_mse_function})\n",
    "else:    \n",
    "    # si no lo hay creamos el modelo desde cero\n",
    "    model = HourglassNetwork(stacks = 4, resnet_per_block = 1)\n",
    "    rms = RMSprop(lr=lr_schedule(0))\n",
    "    model.compile(optimizer=rms, loss=custom_loss_mse_function, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos puntos de guardado del modelo para que guarde solo la mejor version durante el entrenamiento:\n",
    "checkpoint = ModelCheckpoint('single_person_modelvFULLDATA.h5', monitor='re_lu_233_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# modificaciones al learning rate per epoch:\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "# modificaciones al learning rate por estancamiento:\n",
    "lr_reducer = ReduceLROnPlateau(monitor = 'val_re_lu_233_loss',\n",
    "                               factor = 0.1,\n",
    "                               patience = 10,\n",
    "                               min_lr = 0,\n",
    "                              verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Acceso denegado: './logs/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7b6ef71bf169>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Clear any logs from previous runs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./logs/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlog_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"logs/fit/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%d/%m/%Y - %H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlog_callback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistogram_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Acceso denegado: './logs/'"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "#os.remove(\"./logs/\")\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\")\n",
    "log_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agregamos todas las funciones al callback_list\n",
    "callbacks_list = [checkpoint, lr_scheduler, lr_reducer, log_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0001\n",
      "Epoch 1/10\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 13333.8320 - re_lu_65_loss: 3415.3728 - re_lu_121_loss: 3346.9939 - re_lu_177_loss: 3296.0701 - re_lu_233_loss: 3275.3960 - re_lu_65_accuracy: 0.1098 - re_lu_121_accuracy: 0.1277 - re_lu_177_accuracy: 0.2007 - re_lu_233_accuracy: 0.1428WARNING:tensorflow:From c:\\users\\espin\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2/4 [==============>...............] - ETA: 0s - loss: 12667.6797 - re_lu_65_loss: 3242.9033 - re_lu_121_loss: 3182.0564 - re_lu_177_loss: 3138.0908 - re_lu_233_loss: 3104.6292 - re_lu_65_accuracy: 0.1111 - re_lu_121_accuracy: 0.1177 - re_lu_177_accuracy: 0.1748 - re_lu_233_accuracy: 0.1515WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1295s vs `on_train_batch_end` time: 0.7644s). Check your callbacks.\n",
      "4/4 [==============================] - ETA: 0s - loss: 13122.1162 - re_lu_65_loss: 3358.2681 - re_lu_121_loss: 3301.2883 - re_lu_177_loss: 3250.9561 - re_lu_233_loss: 3211.6035 - re_lu_65_accuracy: 0.1013 - re_lu_121_accuracy: 0.1093 - re_lu_177_accuracy: 0.1690 - re_lu_233_accuracy: 0.1483\n",
      "Epoch 00001: loss improved from inf to 13122.11621, saving model to single_person_modelvPRUEBA_TEST_20_DATOS.h5\n",
      "4/4 [==============================] - 18s 5s/step - loss: 13122.1162 - re_lu_65_loss: 3358.2681 - re_lu_121_loss: 3301.2883 - re_lu_177_loss: 3250.9561 - re_lu_233_loss: 3211.6035 - re_lu_65_accuracy: 0.1013 - re_lu_121_accuracy: 0.1093 - re_lu_177_accuracy: 0.1690 - re_lu_233_accuracy: 0.1483 - val_loss: 14398.7783 - val_re_lu_65_loss: 3630.3315 - val_re_lu_121_loss: 3611.6611 - val_re_lu_177_loss: 3597.9180 - val_re_lu_233_loss: 3558.8682 - val_re_lu_65_accuracy: 0.0127 - val_re_lu_121_accuracy: 0.0144 - val_re_lu_177_accuracy: 0.0076 - val_re_lu_233_accuracy: 0.0104\n",
      "Learning rate:  0.0001\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 13094.0156 - re_lu_65_loss: 3354.8696 - re_lu_121_loss: 3294.0601 - re_lu_177_loss: 3242.8364 - re_lu_233_loss: 3202.2507 - re_lu_65_accuracy: 0.1033 - re_lu_121_accuracy: 0.1078 - re_lu_177_accuracy: 0.1689 - re_lu_233_accuracy: 0.1524\n",
      "Epoch 00002: loss improved from 13122.11621 to 13094.01562, saving model to single_person_modelvPRUEBA_TEST_20_DATOS.h5\n",
      "4/4 [==============================] - 12s 3s/step - loss: 13094.0156 - re_lu_65_loss: 3354.8696 - re_lu_121_loss: 3294.0601 - re_lu_177_loss: 3242.8364 - re_lu_233_loss: 3202.2507 - re_lu_65_accuracy: 0.1033 - re_lu_121_accuracy: 0.1078 - re_lu_177_accuracy: 0.1689 - re_lu_233_accuracy: 0.1524 - val_loss: 14432.3789 - val_re_lu_65_loss: 3632.2412 - val_re_lu_121_loss: 3614.4758 - val_re_lu_177_loss: 3605.6362 - val_re_lu_233_loss: 3580.0259 - val_re_lu_65_accuracy: 0.0133 - val_re_lu_121_accuracy: 0.0162 - val_re_lu_177_accuracy: 0.0067 - val_re_lu_233_accuracy: 0.0104\n",
      "Learning rate:  0.0001\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 13030.3438 - re_lu_65_loss: 3348.1396 - re_lu_121_loss: 3277.4243 - re_lu_177_loss: 3224.9568 - re_lu_233_loss: 3179.8228 - re_lu_65_accuracy: 0.1057 - re_lu_121_accuracy: 0.1141 - re_lu_177_accuracy: 0.1805 - re_lu_233_accuracy: 0.1526\n",
      "Epoch 00003: loss improved from 13094.01562 to 13030.34375, saving model to single_person_modelvPRUEBA_TEST_20_DATOS.h5\n",
      "4/4 [==============================] - 12s 3s/step - loss: 13030.3438 - re_lu_65_loss: 3348.1396 - re_lu_121_loss: 3277.4243 - re_lu_177_loss: 3224.9568 - re_lu_233_loss: 3179.8228 - re_lu_65_accuracy: 0.1057 - re_lu_121_accuracy: 0.1141 - re_lu_177_accuracy: 0.1805 - re_lu_233_accuracy: 0.1526 - val_loss: 14422.3398 - val_re_lu_65_loss: 3633.1689 - val_re_lu_121_loss: 3616.0029 - val_re_lu_177_loss: 3601.7627 - val_re_lu_233_loss: 3571.4050 - val_re_lu_65_accuracy: 0.0165 - val_re_lu_121_accuracy: 0.0157 - val_re_lu_177_accuracy: 0.0088 - val_re_lu_233_accuracy: 0.0099\n",
      "Learning rate:  0.0001\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 12956.9873 - re_lu_65_loss: 3337.2146 - re_lu_121_loss: 3260.9248 - re_lu_177_loss: 3201.7566 - re_lu_233_loss: 3157.0920 - re_lu_65_accuracy: 0.1067 - re_lu_121_accuracy: 0.1202 - re_lu_177_accuracy: 0.1829 - re_lu_233_accuracy: 0.1595\n",
      "Epoch 00004: loss improved from 13030.34375 to 12956.98730, saving model to single_person_modelvPRUEBA_TEST_20_DATOS.h5\n",
      "4/4 [==============================] - 12s 3s/step - loss: 12956.9873 - re_lu_65_loss: 3337.2146 - re_lu_121_loss: 3260.9248 - re_lu_177_loss: 3201.7566 - re_lu_233_loss: 3157.0920 - re_lu_65_accuracy: 0.1067 - re_lu_121_accuracy: 0.1202 - re_lu_177_accuracy: 0.1829 - re_lu_233_accuracy: 0.1595 - val_loss: 14441.9766 - val_re_lu_65_loss: 3633.3250 - val_re_lu_121_loss: 3615.6675 - val_re_lu_177_loss: 3610.6113 - val_re_lu_233_loss: 3582.3735 - val_re_lu_65_accuracy: 0.0168 - val_re_lu_121_accuracy: 0.0162 - val_re_lu_177_accuracy: 0.0085 - val_re_lu_233_accuracy: 0.0085\n",
      "Learning rate:  0.0001\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 12907.1016 - re_lu_65_loss: 3331.9319 - re_lu_121_loss: 3248.7898 - re_lu_177_loss: 3186.7380 - re_lu_233_loss: 3139.6421 - re_lu_65_accuracy: 0.1120 - re_lu_121_accuracy: 0.1204 - re_lu_177_accuracy: 0.1923 - re_lu_233_accuracy: 0.1580\n",
      "Epoch 00005: loss improved from 12956.98730 to 12907.10156, saving model to single_person_modelvPRUEBA_TEST_20_DATOS.h5\n",
      "4/4 [==============================] - 14s 3s/step - loss: 12907.1016 - re_lu_65_loss: 3331.9319 - re_lu_121_loss: 3248.7898 - re_lu_177_loss: 3186.7380 - re_lu_233_loss: 3139.6421 - re_lu_65_accuracy: 0.1120 - re_lu_121_accuracy: 0.1204 - re_lu_177_accuracy: 0.1923 - re_lu_233_accuracy: 0.1580 - val_loss: 14478.2129 - val_re_lu_65_loss: 3635.7354 - val_re_lu_121_loss: 3621.7798 - val_re_lu_177_loss: 3621.0139 - val_re_lu_233_loss: 3599.6838 - val_re_lu_65_accuracy: 0.0190 - val_re_lu_121_accuracy: 0.0176 - val_re_lu_177_accuracy: 0.0074 - val_re_lu_233_accuracy: 0.0107\n",
      "Learning rate:  0.0001\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 12901.1641 - re_lu_65_loss: 3335.8784 - re_lu_121_loss: 3246.5508 - re_lu_177_loss: 3183.7168 - re_lu_233_loss: 3135.0178 - re_lu_65_accuracy: 0.1126 - re_lu_121_accuracy: 0.1305 - re_lu_177_accuracy: 0.1868 - re_lu_233_accuracy: 0.1598\n",
      "Epoch 00006: loss improved from 12907.10156 to 12901.16406, saving model to single_person_modelvPRUEBA_TEST_20_DATOS.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "4/4 [==============================] - 13s 3s/step - loss: 12901.1641 - re_lu_65_loss: 3335.8784 - re_lu_121_loss: 3246.5508 - re_lu_177_loss: 3183.7168 - re_lu_233_loss: 3135.0178 - re_lu_65_accuracy: 0.1126 - re_lu_121_accuracy: 0.1305 - re_lu_177_accuracy: 0.1868 - re_lu_233_accuracy: 0.1598 - val_loss: 14480.2949 - val_re_lu_65_loss: 3636.3772 - val_re_lu_121_loss: 3621.0120 - val_re_lu_177_loss: 3621.7095 - val_re_lu_233_loss: 3601.1958 - val_re_lu_65_accuracy: 0.0190 - val_re_lu_121_accuracy: 0.0170 - val_re_lu_177_accuracy: 0.0074 - val_re_lu_233_accuracy: 0.0090\n",
      "Learning rate:  0.0001\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 12846.2715 - re_lu_65_loss: 3326.3853 - re_lu_121_loss: 3233.3140 - re_lu_177_loss: 3168.8291 - re_lu_233_loss: 3117.7424 - re_lu_65_accuracy: 0.1169 - re_lu_121_accuracy: 0.1267 - re_lu_177_accuracy: 0.1949 - re_lu_233_accuracy: 0.1666\n",
      "Epoch 00007: loss improved from 12901.16406 to 12846.27148, saving model to single_person_modelvPRUEBA_TEST_20_DATOS.h5\n",
      "4/4 [==============================] - 13s 3s/step - loss: 12846.2715 - re_lu_65_loss: 3326.3853 - re_lu_121_loss: 3233.3140 - re_lu_177_loss: 3168.8291 - re_lu_233_loss: 3117.7424 - re_lu_65_accuracy: 0.1169 - re_lu_121_accuracy: 0.1267 - re_lu_177_accuracy: 0.1949 - re_lu_233_accuracy: 0.1666 - val_loss: 14503.8115 - val_re_lu_65_loss: 3637.1870 - val_re_lu_121_loss: 3624.4033 - val_re_lu_177_loss: 3625.7131 - val_re_lu_233_loss: 3616.5078 - val_re_lu_65_accuracy: 0.0212 - val_re_lu_121_accuracy: 0.0177 - val_re_lu_177_accuracy: 0.0087 - val_re_lu_233_accuracy: 0.0114\n",
      "Learning rate:  0.0001\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 12769.7695 - re_lu_65_loss: 3312.7283 - re_lu_121_loss: 3214.7957 - re_lu_177_loss: 3146.0781 - re_lu_233_loss: 3096.1689 - re_lu_65_accuracy: 0.1257 - re_lu_121_accuracy: 0.1283 - re_lu_177_accuracy: 0.2015 - re_lu_233_accuracy: 0.1667\n",
      "Epoch 00008: loss improved from 12846.27148 to 12769.76953, saving model to single_person_modelvPRUEBA_TEST_20_DATOS.h5\n",
      "4/4 [==============================] - 12s 3s/step - loss: 12769.7695 - re_lu_65_loss: 3312.7283 - re_lu_121_loss: 3214.7957 - re_lu_177_loss: 3146.0781 - re_lu_233_loss: 3096.1689 - re_lu_65_accuracy: 0.1257 - re_lu_121_accuracy: 0.1283 - re_lu_177_accuracy: 0.2015 - re_lu_233_accuracy: 0.1667 - val_loss: 14503.4531 - val_re_lu_65_loss: 3637.5142 - val_re_lu_121_loss: 3624.4033 - val_re_lu_177_loss: 3628.7119 - val_re_lu_233_loss: 3612.8242 - val_re_lu_65_accuracy: 0.0208 - val_re_lu_121_accuracy: 0.0160 - val_re_lu_177_accuracy: 0.0076 - val_re_lu_233_accuracy: 0.0111\n",
      "Learning rate:  0.0001\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 12735.4199 - re_lu_65_loss: 3312.6235 - re_lu_121_loss: 3206.4517 - re_lu_177_loss: 3135.3223 - re_lu_233_loss: 3081.0234 - re_lu_65_accuracy: 0.1211 - re_lu_121_accuracy: 0.1367 - re_lu_177_accuracy: 0.1927 - re_lu_233_accuracy: 0.1613\n",
      "Epoch 00009: loss improved from 12769.76953 to 12735.41992, saving model to single_person_modelvPRUEBA_TEST_20_DATOS.h5\n",
      "4/4 [==============================] - 13s 3s/step - loss: 12735.4199 - re_lu_65_loss: 3312.6235 - re_lu_121_loss: 3206.4517 - re_lu_177_loss: 3135.3223 - re_lu_233_loss: 3081.0234 - re_lu_65_accuracy: 0.1211 - re_lu_121_accuracy: 0.1367 - re_lu_177_accuracy: 0.1927 - re_lu_233_accuracy: 0.1613 - val_loss: 14507.0059 - val_re_lu_65_loss: 3636.7881 - val_re_lu_121_loss: 3626.4814 - val_re_lu_177_loss: 3626.3579 - val_re_lu_233_loss: 3617.3784 - val_re_lu_65_accuracy: 0.0168 - val_re_lu_121_accuracy: 0.0169 - val_re_lu_177_accuracy: 0.0088 - val_re_lu_233_accuracy: 0.0103\n",
      "Learning rate:  0.0001\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - ETA: 0s - loss: 12701.7920 - re_lu_65_loss: 3308.2744 - re_lu_121_loss: 3197.4841 - re_lu_177_loss: 3124.3164 - re_lu_233_loss: 3071.7168 - re_lu_65_accuracy: 0.1274 - re_lu_121_accuracy: 0.1395 - re_lu_177_accuracy: 0.1885 - re_lu_233_accuracy: 0.1710\n",
      "Epoch 00010: loss improved from 12735.41992 to 12701.79199, saving model to single_person_modelvPRUEBA_TEST_20_DATOS.h5\n",
      "4/4 [==============================] - 13s 3s/step - loss: 12701.7920 - re_lu_65_loss: 3308.2744 - re_lu_121_loss: 3197.4841 - re_lu_177_loss: 3124.3164 - re_lu_233_loss: 3071.7168 - re_lu_65_accuracy: 0.1274 - re_lu_121_accuracy: 0.1395 - re_lu_177_accuracy: 0.1885 - re_lu_233_accuracy: 0.1710 - val_loss: 14508.3086 - val_re_lu_65_loss: 3637.2275 - val_re_lu_121_loss: 3627.5239 - val_re_lu_177_loss: 3626.8638 - val_re_lu_233_loss: 3616.6938 - val_re_lu_65_accuracy: 0.0151 - val_re_lu_121_accuracy: 0.0128 - val_re_lu_177_accuracy: 0.0087 - val_re_lu_233_accuracy: 0.0092\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), batch_size=4, callbacks = callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
