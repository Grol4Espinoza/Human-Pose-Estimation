{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from operator import itemgetter\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import (\n",
    "                                    Dense,\n",
    "                                    Conv2D, \n",
    "                                    BatchNormalization, \n",
    "                                    ReLU, \n",
    "                                    Add,\n",
    "                                    Input,\n",
    "                                    MaxPooling2D,\n",
    "                                    UpSampling2D,\n",
    "                                    )\n",
    "from keras.models import Model\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from math import exp\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "def generate_dataset_obj(obj):\n",
    "    if type(obj) == np.ndarray:\n",
    "        dim = obj.shape[0]\n",
    "        if dim == 1:\n",
    "            ret = generate_dataset_obj(obj[0])             \n",
    "        else:\n",
    "            ret = []\n",
    "            for i in range(dim):\n",
    "                ret.append(generate_dataset_obj(obj[i]))                \n",
    "\n",
    "    elif type(obj) == scipy.io.matlab.mio5_params.mat_struct:\n",
    "        ret = {}\n",
    "        for field_name in obj._fieldnames:            \n",
    "            field = generate_dataset_obj(obj.__dict__[field_name])\n",
    "            if field_name in must_be_list_fields:\n",
    "                field = [field]\n",
    "                ret[field_name] = field\n",
    "\n",
    "    else:\n",
    "        ret = obj\n",
    "\n",
    "    return ret\n",
    "############################################################################\n",
    "def generate_dataset_obj(obj):\n",
    "    if type(obj) == np.ndarray:\n",
    "        dim = obj.shape[0]\n",
    "        if dim == 1:\n",
    "            ret = generate_dataset_obj(obj[0])             \n",
    "        else:\n",
    "            ret = []\n",
    "            for i in range(dim):\n",
    "                ret.append(generate_dataset_obj(obj[i]))                \n",
    "\n",
    "    elif type(obj) == scipy.io.matlab.mio5_params.mat_struct:\n",
    "        ret = {}\n",
    "        for field_name in obj._fieldnames:            \n",
    "            field = generate_dataset_obj(obj.__dict__[field_name])\n",
    "            if field_name in must_be_list_fields:\n",
    "                field = [field]\n",
    "                ret[field_name] = field\n",
    "\n",
    "    else:\n",
    "        ret = obj\n",
    "\n",
    "    return ret\n",
    "\n",
    "############################################################################\n",
    "def print_dataset_obj(obj, depth = 0, maxIterInArray = 20):\n",
    "    prefix = \"  \"*depth\n",
    "    if type(obj) == dict:\n",
    "        for key in obj.keys():\n",
    "            print(\"{}{}\".format(prefix, key))\n",
    "            print_dataset_obj(obj[key], depth + 1)\n",
    "    elif type(obj) == list:\n",
    "        for i, value in enumerate(obj):\n",
    "            if i >= maxIterInArray:\n",
    "                break\n",
    "            print(\"{}{}\".format(prefix, i))\n",
    "            print_dataset_obj(value, depth + 1)\n",
    "    else:\n",
    "        print(\"{}{}\".format(prefix, obj))\n",
    "############################################################################\n",
    "def return_image_joints(name,data):\n",
    "    for item in data: # guardar coordenadas de los joints\n",
    "        if item[0] == name:\n",
    "            #print(item[1]) \n",
    "            return item[1]\n",
    "############################################################################\n",
    "rightconnections = [\n",
    "                    (0,1),(1,2),(3,4),(4,5),(2,6),\n",
    "                    (3,6),(6,7),(7,8),(8,9),(10,11),\n",
    "                    (11,12),(12,7),(13,7),(13,14),(14,15)\n",
    "                   ]\n",
    "size_img_x = 256\n",
    "size_img_y = 256\n",
    "def draw_img_joints(file_name, data, resize = False ):    \n",
    "    # Load image\n",
    "    #img = cv2.imread(Path_To_Single_Person_Images + \"/\" + file_name,1)  \n",
    "    img = image.load_img(Path_To_Single_Person_Images + \"/\" + file_name)\n",
    "    img = image.img_to_array(img) \n",
    "    img = img/255\n",
    "    if resize:\n",
    "        img = np.float32(tf.image.resize(img,(size_img_x, size_img_y)))  \n",
    "    pts = return_image_joints(file_name, data)        \n",
    "    #plt.imshow(img)  \n",
    "    X = [x[0] for x in pts]\n",
    "    Y = [y[1] for y in pts]\n",
    "    X = [int(x) for x in X]\n",
    "    Y = [int(y) for y in Y]\n",
    "    \n",
    "    for i in range(16):\n",
    "        for j in range(16):\n",
    "            if (i,j) in rightconnections:\n",
    "                if X[i]>0 and X[j]>0 and Y[i]>0 and Y[j]>0:\n",
    "                    img = cv2.line(img,(X[i],Y[i]),(X[j],Y[j]),(1,0,0),5)\n",
    "                    plt.scatter(X[i], Y[i], marker=\"o\", color=\"red\", s=20)\n",
    "                    plt.scatter(X[j], Y[j], marker=\"o\", color=\"red\", s=20)\n",
    "                    \n",
    "    plt.imshow(img)\n",
    "############################################################################\n",
    "def load_image(train_data, a, b):\n",
    "    train = np.asarray(train_data[a:b])\n",
    "    train_image = np.zeros((b-a,size_img_x,size_img_y,3))\n",
    "    for i in tqdm(range(a,b)):\n",
    "        name_img = train[i][0]\n",
    "        img = image.load_img(Path_To_Single_Person_Images + '/' + name_img)\n",
    "        img = image.img_to_array(img)\n",
    "        img_x = img.shape[1]\n",
    "        img_y = img.shape[0]\n",
    "        scala_x = img_x / size_img_x\n",
    "        scala_y = img_y / size_img_y\n",
    "        for j in range(len(train[i][1])): # escala los puntos clave\n",
    "            train[i][1][j] = np.array([train[i][1][j][0] / scala_x, train[i][1][j][1] / scala_y])            \n",
    "        img = tf.image.resize(img,(size_img_x, size_img_y))        \n",
    "        img = img/255\n",
    "        train_image[i] = img\n",
    "    return train_image, train\n",
    "############################################################################\n",
    "def MakeHeatmap(x, y, width, height, show = False):\n",
    "    # Probability as a function of distance from the center derived\n",
    "    # from a gaussian distribution with mean = 0 and stdv = 1\n",
    "    scaledGaussian = lambda x : exp(-(1/2)*(x**2))\n",
    "\n",
    "    imgSize = (height, width)\n",
    "    center_x = x\n",
    "    center_y = y\n",
    "\n",
    "    isotropicGrayscaleImage = np.zeros((imgSize[0],imgSize[1]),np.uint8)\n",
    "\n",
    "    for i in range(imgSize[0]):\n",
    "        for j in range(imgSize[1]):\n",
    "\n",
    "            # find euclidian distance from center of image (x,y) \n",
    "            # and scale it to range of 0 to 2.5 as scaled Gaussian\n",
    "            # returns highest probability for x=0 and approximately\n",
    "            # zero probability for x > 2.5\n",
    "\n",
    "            distanceFromCenter = np.linalg.norm(np.array([i-center_y,j-center_x]))\n",
    "            #distanceFromCenter = 18*distanceFromCenter/(imgSize/2)\n",
    "            scaledGaussianProb = scaledGaussian(distanceFromCenter)\n",
    "            isotropicGrayscaleImage[i,j] = np.clip(scaledGaussianProb*255,0,255)   \n",
    "    \n",
    "    return isotropicGrayscaleImage\n",
    "############################################################################    \n",
    "def Joints_heatmaps(lista_de_joints, heatmap_size_x, heatmap_size_y, num_heatmaps = 16, show = False):\n",
    "    heatmaps = np.zeros((16,64,64))\n",
    "    for i in range(num_heatmaps):\n",
    "        x, y = lista_de_joints[i] \n",
    "        x = x / 4 # entre 4 por que el array es de 256x256\n",
    "        y = y / 4 # entre 4 por que el array es de 256x256\n",
    "        heatmaps[i] = MakeHeatmap(x, y, heatmap_size_x, heatmap_size_y)\n",
    "    if show:\n",
    "        plotImages(heatmaps, num_heatmaps)\n",
    "    return heatmaps\n",
    "############################################################################        \n",
    "def plotImages(images_arr, num_images):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "############################################################################    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar/cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('lista_de_heatmaps.npy'):\n",
    "    #Ahora cargamos las imagenes\n",
    "    lista_de_heatmaps = np.load('lista_de_heatmaps.npy')   \n",
    "else: \n",
    "    if os.path.isfile('lista_de_imagenes.npy') and os.path.isfile('lista_de_joints.npy'):\n",
    "        #Ahora cargamos las imagenes\n",
    "        lista_de_joints = np.load('lista_de_joints.npy',allow_pickle=True)\n",
    "        lista_de_imagenes = np.load('lista_de_imagenes.npy')    \n",
    "    else: \n",
    "        #darle formato de diccionario\n",
    "        matph = './mpii.mat'\n",
    "        decoded1 = loadmat(matph, struct_as_record=False)[\"RELEASE\"]\n",
    "        must_be_list_fields = [\"annolist\",\"image\",\"name\", \"annorect\", \"scale\", \"x\", \"y\", \"annopoints\", \"point\", \"id\"]\n",
    "        # Convert to dict\n",
    "        dataset_obj = generate_dataset_obj(decoded1)\n",
    "        # Print it out\n",
    "        #print_dataset_obj(dataset_obj)\n",
    "        len(dataset_obj['annolist'][0])\n",
    "        #solo queremos la información en 'annolist'\n",
    "        dataset = dataset_obj['annolist'][0]\n",
    "        #guardamos solo informacion de las imagenes que tienen solo una persona\n",
    "        train_data = []\n",
    "        for i in range(len(dataset)):\n",
    "            if \"annopoints\" in dataset[i]['annorect'][0]:     \n",
    "                name = dataset[i]['image'][0]['name'][0]\n",
    "                tupla = [] \n",
    "                for j in range(16):     #ordena los puntos de articulaciones del id = 0 al id = 15       \n",
    "                    try:\n",
    "                        x = dataset[i]['annorect'][0]['annopoints'][0]['point'][0][j]['x'][0]\n",
    "                    except:\n",
    "                        x = -1\n",
    "                    try:\n",
    "                        y = dataset[i]['annorect'][0]['annopoints'][0]['point'][0][j]['y'][0]\n",
    "                    except:\n",
    "                        y = -1\n",
    "                    try:\n",
    "                        id = dataset[i]['annorect'][0]['annopoints'][0]['point'][0][j]['id'][0]\n",
    "                    except:\n",
    "                        id = -1          \n",
    "                    tupla.append((x,y,id))\n",
    "                tupla = sorted(tupla, key = itemgetter(2)) # esto lo ordena\n",
    "                for j in range(len(tupla)):   #quita id de las tuplas,\n",
    "                    tupla[j] = tupla[j][:2]\n",
    "                #pasa de tupla a array\n",
    "                tupla = np.asarray(tupla)        \n",
    "                train_data.append((name,tupla))\n",
    "        #Creamos un array para guardar los nombres        \n",
    "        names = []\n",
    "        for item in train_data:#guardar nombres de las imagenes que voy a usar en \"name\"\n",
    "            names.append(item[0])\n",
    "\n",
    "        #Crear Carpeta para guardar imagenes del dataset\n",
    "        Path_To_Raw_Images = 'DataSet/mpii_human_pose_v1_images'\n",
    "        Path_To_Single_Person_Images = 'DataSet/mpii_human_pose_v1_images/SinglePersonImagesWithData'\n",
    "        os.chdir(Path_To_Raw_Images)\n",
    "\n",
    "        if os.path.isdir('SinglePersonImagesWithData') is False:\n",
    "            os.makedirs('SinglePersonImagesWithData')\n",
    "            for images in names:\n",
    "                shutil.move(images, 'SinglePersonImagesWithData')\n",
    "\n",
    "        os.chdir('../../')\n",
    "\n",
    "        #Demostración dibujar joints en imagenes con data\n",
    "        #draw_img_joints('060111501.jpg',train_data)\n",
    "\n",
    "        #Ahora cargamos las imagenes\n",
    "        lista_de_imagenes, lista_de_joints = load_image(train_data,0,5000)\n",
    "        np.save('lista_de_imagenes', lista_de_imagenes)\n",
    "        np.save('lista_de_joints', lista_de_joints)\n",
    "\n",
    "    #lista_de_joints.shape\n",
    "    #draw_img_joints(lista_de_joints[17][0],lista_de_joints, resize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 256, 256, 3) (5000, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(lista_de_imagenes.shape,lista_de_imagenes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5000/5000 [3:23:51<00:00,  2.45s/it]\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('lista_de_heatmaps.npy'):\n",
    "    #Ahora cargamos las imagenes\n",
    "    lista_de_heatmaps = np.load('lista_de_heatmaps.npy')   \n",
    "else: \n",
    "    heatmap_size_x = 64\n",
    "    heatmap_size_y = 64 \n",
    "    #dibujo_test_heatmaps = Joints_heatmaps(lista_de_joints[0][1], heatmap_size_x, heatmap_size_y, show = False)\n",
    "    #creamos los heatmaps de nuestra data\n",
    "    lista_de_heatmaps = np.zeros((5000,64,64,16))\n",
    "    for i in tqdm(range(lista_de_joints.shape[0])):\n",
    "        joints = return_image_joints(lista_de_joints[i][0], lista_de_joints)\n",
    "        lista_de_heatmaps[i] = np.moveaxis(Joints_heatmaps(joints, heatmap_size_x, heatmap_size_y), 0, -1) # change shape from 16x64x64 to 64x64x16\n",
    "\n",
    "    #guardamos el array    \n",
    "    np.save('lista_de_heatmaps', lista_de_heatmaps)\n",
    "    #plotImages(lista_de_heatmaps[67], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_de_heatmaps[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D(inputs, \n",
    "           filters, \n",
    "           kernel_size = 1,\n",
    "           strides = 1,\n",
    "           padding = 'same',\n",
    "           kernel_initializer = 'he_normal',\n",
    "           activation = True,\n",
    "           batch_normalization = True,\n",
    "           name = \"conv\"):\n",
    "    \n",
    "    x = Conv2D(filters, kernel_size, strides=strides, padding=padding,\n",
    "            use_bias=False, kernel_initializer=kernel_initializer)(inputs)\n",
    "    if batch_normalization:\n",
    "        x = BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = ReLU()(x)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arquitectura ResNetV2 de cuello de botella\n",
    "def ResNetV2(inputs, filters, strides = 1, lift_channels = False, name = 'bloque'):\n",
    "    \n",
    "    res = inputs\n",
    "    #incrementa el número de canales si es necesario\n",
    "    if lift_channels:\n",
    "        res = conv2D(\n",
    "            inputs,\n",
    "            filters,\n",
    "            activation = False,\n",
    "            batch_normalization = False)\n",
    "    \n",
    "    x = BatchNormalization()(inputs)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    #conv de tamaño 1x1\n",
    "    x = conv2D(\n",
    "        x,\n",
    "        filters = filters/2)\n",
    "    \n",
    "    #conv de tamaño 3x3\n",
    "    x = conv2D(\n",
    "        x,\n",
    "        filters = filters/2,\n",
    "        kernel_size = 3)\n",
    "    \n",
    "    #conv de tamaño 1x1\n",
    "    x = conv2D(\n",
    "        x,\n",
    "        filters = filters,\n",
    "        activation = False,\n",
    "        batch_normalization = False)\n",
    "    \n",
    "    x = Add()([res,x])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassUnit(inputs, depth, filters, resnet_per_block, name = 'hourglass_'):    \n",
    "    \n",
    "    #Capas \"superiores\"\n",
    "    up_1 = ResNetV2(inputs,filters)\n",
    "    \n",
    "    for i in range(resnet_per_block):\n",
    "        up_1 = ResNetV2(up_1, filters)\n",
    "    \n",
    "    #Capas \"inferiores\"\n",
    "    #Reducir resolución\n",
    "    low_1 = MaxPooling2D(pool_size = 2, strides = 2)(inputs)\n",
    "    \n",
    "    for i in range(resnet_per_block):\n",
    "        low_1 = ResNetV2(low_1, filters)\n",
    "    \n",
    "    low_2 = low_1\n",
    "    if depth > 1 : \n",
    "        low_2 = HourglassUnit(low_1, depth-1, filters, resnet_per_block)\n",
    "    else:\n",
    "        low_2 = ResNetV2(low_2, filters)\n",
    "    \n",
    "    low_3 = low_2\n",
    "    \n",
    "    for i in range(resnet_per_block):\n",
    "        low_3 = ResNetV2(low_3, filters)\n",
    "    \n",
    "    #Aumentar resolución\n",
    "    up_2 = UpSampling2D()(low_3)\n",
    "    \n",
    "    return Add()([up_1,up_2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassNetwork(input_shape = (256,256,3), stacks = 8, resnet_per_block = 3, heatmaps = 16):\n",
    "    \n",
    "    inputs = Input(shape = input_shape)\n",
    "    \n",
    "    #la data llega en formato 256x256x3, la pasamos a 64x64x256\n",
    "    #preprocessing\n",
    "    #amplia canales a 64\n",
    "    x = conv2D(\n",
    "        inputs,\n",
    "        filters = 64,\n",
    "        kernel_size = 7,\n",
    "        strides = 2)\n",
    "    #amplia canales de 64 a 128 \n",
    "    x = ResNetV2(x, filters = 128, lift_channels = True)\n",
    "    x = MaxPooling2D(pool_size = 2, strides = 2)(x)\n",
    "    x = ResNetV2(x, filters = 128)\n",
    "    #amplia canales de 64 a 128\n",
    "    x = ResNetV2(x, filters = 256, lift_channels = True)\n",
    "    skip = x\n",
    "    y_heatmaps = []\n",
    "    \n",
    "    for i in range(stacks):\n",
    "        x = HourglassUnit(x, 4, filters = 256, resnet_per_block = resnet_per_block)\n",
    "        \n",
    "        x = ResNetV2(x, filters = 256)\n",
    "        \n",
    "        #prediccion de 256 canales \n",
    "        x = conv2D(x, filters = 256)\n",
    "        \n",
    "        #prediccion temporal de heatmaps\n",
    "        y = conv2D(x, filters = heatmaps)\n",
    "        #agregamos el resultado temportal al array de resultados para la supervision intermedia\n",
    "        y_heatmaps.append(y)\n",
    "        \n",
    "        #ahora regresamos el tensor y al orden de 256 canales si es que no es el ultimo output\n",
    "        if i < stacks - 1:\n",
    "            y_recovery1 = conv2D(x, filters = 256, activation = False, batch_normalization = False)\n",
    "            y_recovery2 = conv2D(y, filters = 256, activation = False, batch_normalization = False)\n",
    "            x = Add()([skip, y_recovery1, y_recovery2])\n",
    "    #print(y_heatmaps)\n",
    "    return Model(inputs = inputs, outputs = y_heatmaps, name = 'HourglassNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasa de decrecimiento del learning rate por numero de epoch\n",
    "def lr_schedule(epoch): #tome esto de resnet.ipynb\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 5e-4\n",
    "    if epoch > 180:\n",
    "        lr *= 5e-4\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr \n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicio el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HourglassNetwork(stacks = 4, resnet_per_block = 1)\n",
    "rms = RMSprop(lr=lr_schedule(0))\n",
    "model.compile(optimizer=rms, loss=mean_squared_error, metrics=[\"accuracy\"])\n",
    "#model.save('single_person_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(lista_de_imagenes, lista_de_heatmaps, random_state=7, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 256, 256, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.imshow(X_train[0])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 64, 64, 16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save \n",
    "np.save('X_train', X_train)\n",
    "np.save('X_test', X_test)\n",
    "np.save('y_train', y_train)\n",
    "np.save('y_test', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "X_train = np.load('X_train.npy', mmap_mode='r')\n",
    "X_test = np.load('X_test.npy', mmap_mode='r')\n",
    "y_train = np.load('y_train.npy', mmap_mode='r')\n",
    "y_test = np.load('y_test.npy', mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos puntos de guardado del modelo para que guarde solo la mejor version durante el entrenamiento:\n",
    "checkpoint = ModelCheckpoint('single_person_model.h5', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# modificaciones al learning rate per epoch:\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "# modificaciones al learning rate por estancamiento:\n",
    "lr_reducer = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                               factor = 0.1,\n",
    "                               patience = 15,\n",
    "                               min_lr = 0)\n",
    "# agregamos estas funciones al callback_list\n",
    "callbacks_list = [checkpoint, lr_scheduler, lr_reducer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 179.4997 - re_lu_65_loss: 45.3994 - re_lu_121_loss: 44.8552 - re_lu_177_loss: 44.6679 - re_lu_233_loss: 44.5770 - re_lu_65_accuracy: 0.1085 - re_lu_121_accuracy: 0.0916 - re_lu_177_accuracy: 0.1021 - re_lu_233_accuracy: 0.0862WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0235s vs `on_test_batch_end` time: 0.0575s). Check your callbacks.\n",
      "1000/1000 [==============================] - 309s 309ms/step - loss: 179.4997 - re_lu_65_loss: 45.3994 - re_lu_121_loss: 44.8552 - re_lu_177_loss: 44.6679 - re_lu_233_loss: 44.5770 - re_lu_65_accuracy: 0.1085 - re_lu_121_accuracy: 0.0916 - re_lu_177_accuracy: 0.1021 - re_lu_233_accuracy: 0.0862 - val_loss: 175.1575 - val_re_lu_65_loss: 44.7756 - val_re_lu_121_loss: 43.9960 - val_re_lu_177_loss: 43.3738 - val_re_lu_233_loss: 43.0120 - val_re_lu_65_accuracy: 0.0454 - val_re_lu_121_accuracy: 0.0443 - val_re_lu_177_accuracy: 0.1305 - val_re_lu_233_accuracy: 0.0572\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 223s 223ms/step - loss: 170.4483 - re_lu_65_loss: 43.6260 - re_lu_121_loss: 42.5457 - re_lu_177_loss: 42.2239 - re_lu_233_loss: 42.0527 - re_lu_65_accuracy: 0.0729 - re_lu_121_accuracy: 0.0747 - re_lu_177_accuracy: 0.0772 - re_lu_233_accuracy: 0.0719 - val_loss: 171.4829 - val_re_lu_65_loss: 43.8327 - val_re_lu_121_loss: 42.6491 - val_re_lu_177_loss: 42.5318 - val_re_lu_233_loss: 42.4693 - val_re_lu_65_accuracy: 0.0701 - val_re_lu_121_accuracy: 0.0699 - val_re_lu_177_accuracy: 0.0840 - val_re_lu_233_accuracy: 0.0834\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 224s 224ms/step - loss: 163.2469 - re_lu_65_loss: 42.2784 - re_lu_121_loss: 40.7672 - re_lu_177_loss: 40.2427 - re_lu_233_loss: 39.9586 - re_lu_65_accuracy: 0.0834 - re_lu_121_accuracy: 0.0787 - re_lu_177_accuracy: 0.0809 - re_lu_233_accuracy: 0.0800 - val_loss: 164.8902 - val_re_lu_65_loss: 42.6388 - val_re_lu_121_loss: 41.1786 - val_re_lu_177_loss: 40.8365 - val_re_lu_233_loss: 40.2363 - val_re_lu_65_accuracy: 0.0532 - val_re_lu_121_accuracy: 0.0609 - val_re_lu_177_accuracy: 0.0853 - val_re_lu_233_accuracy: 0.1419\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 226s 226ms/step - loss: 156.3381 - re_lu_65_loss: 41.1224 - re_lu_121_loss: 39.0808 - re_lu_177_loss: 38.2744 - re_lu_233_loss: 37.8605 - re_lu_65_accuracy: 0.0802 - re_lu_121_accuracy: 0.0807 - re_lu_177_accuracy: 0.0840 - re_lu_233_accuracy: 0.0811 - val_loss: 160.7358 - val_re_lu_65_loss: 41.7995 - val_re_lu_121_loss: 40.0822 - val_re_lu_177_loss: 39.5938 - val_re_lu_233_loss: 39.2603 - val_re_lu_65_accuracy: 0.0757 - val_re_lu_121_accuracy: 0.0705 - val_re_lu_177_accuracy: 0.0517 - val_re_lu_233_accuracy: 0.0484\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 225s 225ms/step - loss: 149.5116 - re_lu_65_loss: 40.0603 - re_lu_121_loss: 37.3935 - re_lu_177_loss: 36.2974 - re_lu_233_loss: 35.7603 - re_lu_65_accuracy: 0.0779 - re_lu_121_accuracy: 0.0813 - re_lu_177_accuracy: 0.0844 - re_lu_233_accuracy: 0.0837 - val_loss: 156.4096 - val_re_lu_65_loss: 41.4406 - val_re_lu_121_loss: 39.1448 - val_re_lu_177_loss: 38.1501 - val_re_lu_233_loss: 37.6741 - val_re_lu_65_accuracy: 0.1121 - val_re_lu_121_accuracy: 0.0912 - val_re_lu_177_accuracy: 0.0786 - val_re_lu_233_accuracy: 0.0578\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 226s 226ms/step - loss: 142.9970 - re_lu_65_loss: 39.0808 - re_lu_121_loss: 35.7825 - re_lu_177_loss: 34.3986 - re_lu_233_loss: 33.7352 - re_lu_65_accuracy: 0.0814 - re_lu_121_accuracy: 0.0874 - re_lu_177_accuracy: 0.0901 - re_lu_233_accuracy: 0.0882 - val_loss: 155.9801 - val_re_lu_65_loss: 41.4443 - val_re_lu_121_loss: 38.8280 - val_re_lu_177_loss: 38.0582 - val_re_lu_233_loss: 37.6496 - val_re_lu_65_accuracy: 0.1779 - val_re_lu_121_accuracy: 0.0856 - val_re_lu_177_accuracy: 0.1273 - val_re_lu_233_accuracy: 0.0998\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 225s 225ms/step - loss: 136.7419 - re_lu_65_loss: 38.2750 - re_lu_121_loss: 34.2180 - re_lu_177_loss: 32.5260 - re_lu_233_loss: 31.7229 - re_lu_65_accuracy: 0.0824 - re_lu_121_accuracy: 0.0856 - re_lu_177_accuracy: 0.0849 - re_lu_233_accuracy: 0.0866 - val_loss: 153.5277 - val_re_lu_65_loss: 40.4735 - val_re_lu_121_loss: 38.1556 - val_re_lu_177_loss: 37.6157 - val_re_lu_233_loss: 37.2830 - val_re_lu_65_accuracy: 0.0519 - val_re_lu_121_accuracy: 0.0960 - val_re_lu_177_accuracy: 0.0522 - val_re_lu_233_accuracy: 0.0551\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 226s 226ms/step - loss: 130.9956 - re_lu_65_loss: 37.5292 - re_lu_121_loss: 32.7415 - re_lu_177_loss: 30.8053 - re_lu_233_loss: 29.9197 - re_lu_65_accuracy: 0.0917 - re_lu_121_accuracy: 0.0948 - re_lu_177_accuracy: 0.0878 - re_lu_233_accuracy: 0.0863 - val_loss: 154.1320 - val_re_lu_65_loss: 40.8149 - val_re_lu_121_loss: 38.3063 - val_re_lu_177_loss: 37.5704 - val_re_lu_233_loss: 37.4403 - val_re_lu_65_accuracy: 0.0691 - val_re_lu_121_accuracy: 0.0888 - val_re_lu_177_accuracy: 0.0765 - val_re_lu_233_accuracy: 0.0749\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 227s 227ms/step - loss: 125.3310 - re_lu_65_loss: 36.8666 - re_lu_121_loss: 31.3158 - re_lu_177_loss: 29.0761 - re_lu_233_loss: 28.0727 - re_lu_65_accuracy: 0.0943 - re_lu_121_accuracy: 0.1010 - re_lu_177_accuracy: 0.0857 - re_lu_233_accuracy: 0.0875 - val_loss: 147.6853 - val_re_lu_65_loss: 39.7266 - val_re_lu_121_loss: 36.6827 - val_re_lu_177_loss: 35.7609 - val_re_lu_233_loss: 35.5151 - val_re_lu_65_accuracy: 0.0848 - val_re_lu_121_accuracy: 0.0849 - val_re_lu_177_accuracy: 0.0678 - val_re_lu_233_accuracy: 0.0553\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 233s 233ms/step - loss: 119.8432 - re_lu_65_loss: 36.1787 - re_lu_121_loss: 29.8794 - re_lu_177_loss: 27.4389 - re_lu_233_loss: 26.3462 - re_lu_65_accuracy: 0.0949 - re_lu_121_accuracy: 0.1070 - re_lu_177_accuracy: 0.0943 - re_lu_233_accuracy: 0.0886 - val_loss: 154.8723 - val_re_lu_65_loss: 41.0532 - val_re_lu_121_loss: 38.5938 - val_re_lu_177_loss: 37.6854 - val_re_lu_233_loss: 37.5398 - val_re_lu_65_accuracy: 0.0769 - val_re_lu_121_accuracy: 0.0880 - val_re_lu_177_accuracy: 0.0976 - val_re_lu_233_accuracy: 0.0546\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 233s 233ms/step - loss: 115.1118 - re_lu_65_loss: 35.6274 - re_lu_121_loss: 28.6578 - re_lu_177_loss: 26.0073 - re_lu_233_loss: 24.8193 - re_lu_65_accuracy: 0.1144 - re_lu_121_accuracy: 0.1178 - re_lu_177_accuracy: 0.1033 - re_lu_233_accuracy: 0.0985 - val_loss: 147.4952 - val_re_lu_65_loss: 39.7006 - val_re_lu_121_loss: 36.5630 - val_re_lu_177_loss: 35.7355 - val_re_lu_233_loss: 35.4961 - val_re_lu_65_accuracy: 0.1401 - val_re_lu_121_accuracy: 0.0727 - val_re_lu_177_accuracy: 0.1330 - val_re_lu_233_accuracy: 0.0916\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 225s 225ms/step - loss: 110.0592 - re_lu_65_loss: 34.9841 - re_lu_121_loss: 27.2888 - re_lu_177_loss: 24.5107 - re_lu_233_loss: 23.2755 - re_lu_65_accuracy: 0.1104 - re_lu_121_accuracy: 0.1315 - re_lu_177_accuracy: 0.1072 - re_lu_233_accuracy: 0.0927 - val_loss: 148.3877 - val_re_lu_65_loss: 39.3695 - val_re_lu_121_loss: 36.9056 - val_re_lu_177_loss: 36.0591 - val_re_lu_233_loss: 36.0535 - val_re_lu_65_accuracy: 0.0878 - val_re_lu_121_accuracy: 0.1167 - val_re_lu_177_accuracy: 0.1016 - val_re_lu_233_accuracy: 0.0791\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 225s 225ms/step - loss: 105.5465 - re_lu_65_loss: 34.4255 - re_lu_121_loss: 26.1141 - re_lu_177_loss: 23.1800 - re_lu_233_loss: 21.8269 - re_lu_65_accuracy: 0.1045 - re_lu_121_accuracy: 0.1467 - re_lu_177_accuracy: 0.1244 - re_lu_233_accuracy: 0.1019 - val_loss: 146.5995 - val_re_lu_65_loss: 39.2262 - val_re_lu_121_loss: 36.3401 - val_re_lu_177_loss: 35.6217 - val_re_lu_233_loss: 35.4114 - val_re_lu_65_accuracy: 0.0819 - val_re_lu_121_accuracy: 0.1476 - val_re_lu_177_accuracy: 0.1040 - val_re_lu_233_accuracy: 0.1383\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 232s 232ms/step - loss: 101.4909 - re_lu_65_loss: 33.9386 - re_lu_121_loss: 25.0430 - re_lu_177_loss: 21.9549 - re_lu_233_loss: 20.5545 - re_lu_65_accuracy: 0.1043 - re_lu_121_accuracy: 0.1438 - re_lu_177_accuracy: 0.1443 - re_lu_233_accuracy: 0.1112 - val_loss: 149.3366 - val_re_lu_65_loss: 38.9488 - val_re_lu_121_loss: 36.8511 - val_re_lu_177_loss: 36.5496 - val_re_lu_233_loss: 36.9871 - val_re_lu_65_accuracy: 0.0790 - val_re_lu_121_accuracy: 0.1577 - val_re_lu_177_accuracy: 0.1445 - val_re_lu_233_accuracy: 0.0987\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 234s 234ms/step - loss: 97.7064 - re_lu_65_loss: 33.4597 - re_lu_121_loss: 24.0194 - re_lu_177_loss: 20.8544 - re_lu_233_loss: 19.3729 - re_lu_65_accuracy: 0.1139 - re_lu_121_accuracy: 0.1611 - re_lu_177_accuracy: 0.1732 - re_lu_233_accuracy: 0.1293 - val_loss: 147.0356 - val_re_lu_65_loss: 38.9773 - val_re_lu_121_loss: 36.2627 - val_re_lu_177_loss: 35.8432 - val_re_lu_233_loss: 35.9524 - val_re_lu_65_accuracy: 0.0927 - val_re_lu_121_accuracy: 0.1437 - val_re_lu_177_accuracy: 0.1787 - val_re_lu_233_accuracy: 0.1451\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 234s 234ms/step - loss: 94.0058 - re_lu_65_loss: 32.9764 - re_lu_121_loss: 23.0318 - re_lu_177_loss: 19.7855 - re_lu_233_loss: 18.2122 - re_lu_65_accuracy: 0.1210 - re_lu_121_accuracy: 0.1776 - re_lu_177_accuracy: 0.1893 - re_lu_233_accuracy: 0.1549 - val_loss: 146.5592 - val_re_lu_65_loss: 38.8449 - val_re_lu_121_loss: 36.0527 - val_re_lu_177_loss: 35.6125 - val_re_lu_233_loss: 36.0491 - val_re_lu_65_accuracy: 0.0765 - val_re_lu_121_accuracy: 0.1405 - val_re_lu_177_accuracy: 0.1662 - val_re_lu_233_accuracy: 0.1959\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 234s 234ms/step - loss: 90.2847 - re_lu_65_loss: 32.4255 - re_lu_121_loss: 22.0461 - re_lu_177_loss: 18.7057 - re_lu_233_loss: 17.1074 - re_lu_65_accuracy: 0.1233 - re_lu_121_accuracy: 0.1944 - re_lu_177_accuracy: 0.2163 - re_lu_233_accuracy: 0.1799 - val_loss: 150.5513 - val_re_lu_65_loss: 39.2269 - val_re_lu_121_loss: 37.0241 - val_re_lu_177_loss: 36.9892 - val_re_lu_233_loss: 37.3111 - val_re_lu_65_accuracy: 0.0992 - val_re_lu_121_accuracy: 0.1678 - val_re_lu_177_accuracy: 0.1888 - val_re_lu_233_accuracy: 0.2461\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 234s 234ms/step - loss: 86.8275 - re_lu_65_loss: 31.9897 - re_lu_121_loss: 21.1021 - re_lu_177_loss: 17.7067 - re_lu_233_loss: 16.0290 - re_lu_65_accuracy: 0.1254 - re_lu_121_accuracy: 0.2156 - re_lu_177_accuracy: 0.2627 - re_lu_233_accuracy: 0.2245 - val_loss: 151.3797 - val_re_lu_65_loss: 39.1406 - val_re_lu_121_loss: 37.0779 - val_re_lu_177_loss: 37.2996 - val_re_lu_233_loss: 37.8617 - val_re_lu_65_accuracy: 0.1160 - val_re_lu_121_accuracy: 0.2224 - val_re_lu_177_accuracy: 0.2742 - val_re_lu_233_accuracy: 0.2319\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 242s 242ms/step - loss: 83.7081 - re_lu_65_loss: 31.5290 - re_lu_121_loss: 20.3052 - re_lu_177_loss: 16.7959 - re_lu_233_loss: 15.0781 - re_lu_65_accuracy: 0.1337 - re_lu_121_accuracy: 0.2438 - re_lu_177_accuracy: 0.2869 - re_lu_233_accuracy: 0.2470 - val_loss: 150.0734 - val_re_lu_65_loss: 38.9014 - val_re_lu_121_loss: 36.6490 - val_re_lu_177_loss: 36.8933 - val_re_lu_233_loss: 37.6297 - val_re_lu_65_accuracy: 0.1136 - val_re_lu_121_accuracy: 0.2417 - val_re_lu_177_accuracy: 0.2785 - val_re_lu_233_accuracy: 0.2777\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 240s 240ms/step - loss: 80.8694 - re_lu_65_loss: 31.0789 - re_lu_121_loss: 19.5516 - re_lu_177_loss: 16.0020 - re_lu_233_loss: 14.2369 - re_lu_65_accuracy: 0.1376 - re_lu_121_accuracy: 0.2665 - re_lu_177_accuracy: 0.3240 - re_lu_233_accuracy: 0.2722 - val_loss: 148.6713 - val_re_lu_65_loss: 38.4071 - val_re_lu_121_loss: 36.6400 - val_re_lu_177_loss: 36.5464 - val_re_lu_233_loss: 37.0778 - val_re_lu_65_accuracy: 0.1083 - val_re_lu_121_accuracy: 0.2478 - val_re_lu_177_accuracy: 0.2414 - val_re_lu_233_accuracy: 0.3766\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=4, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('X_train.npy') and os.path.isfile('X_test.npy') and os.path.isfile('y_train.npy') and os.path.isfile('y_test.npy'):\n",
    "    #Primero comprobamos si existen ya datos para usar en el modelo, si hay los mapeamos, no se cargan a la ram, se leen desde disco \n",
    "    X_train = np.load('X_train.npy', mmap_mode='r')\n",
    "    X_test = np.load('X_test.npy', mmap_mode='r')\n",
    "    y_train = np.load('y_train.npy', mmap_mode='r')\n",
    "    y_test = np.load('y_test.npy', mmap_mode='r')\n",
    "else: \n",
    "    # si no existen entonces iniciamos el preprocesado:    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
