{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from operator import itemgetter\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import (\n",
    "                                    Dense,\n",
    "                                    Conv2D, \n",
    "                                    BatchNormalization, \n",
    "                                    ReLU, \n",
    "                                    Add,\n",
    "                                    Input,\n",
    "                                    MaxPooling2D,\n",
    "                                    UpSampling2D,\n",
    "                                    )\n",
    "from keras.models import Model, load_model\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "from math import exp\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de Datos/Preprocesamiento del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################################################\n",
    "def generate_dataset_obj(obj):\n",
    "    if type(obj) == np.ndarray:\n",
    "        dim = obj.shape[0]\n",
    "        if dim == 1:\n",
    "            ret = generate_dataset_obj(obj[0])             \n",
    "        else:\n",
    "            ret = []\n",
    "            for i in range(dim):\n",
    "                ret.append(generate_dataset_obj(obj[i]))                \n",
    "\n",
    "    elif type(obj) == scipy.io.matlab.mio5_params.mat_struct:\n",
    "        ret = {}\n",
    "        for field_name in obj._fieldnames:            \n",
    "            field = generate_dataset_obj(obj.__dict__[field_name])\n",
    "            if field_name in must_be_list_fields:\n",
    "                field = [field]\n",
    "                ret[field_name] = field\n",
    "\n",
    "    else:\n",
    "        ret = obj\n",
    "\n",
    "    return ret\n",
    "########################################################################################################################################################\n",
    "def generate_dataset_obj(obj):\n",
    "    if type(obj) == np.ndarray:\n",
    "        dim = obj.shape[0]\n",
    "        if dim == 1:\n",
    "            ret = generate_dataset_obj(obj[0])             \n",
    "        else:\n",
    "            ret = []\n",
    "            for i in range(dim):\n",
    "                ret.append(generate_dataset_obj(obj[i]))                \n",
    "\n",
    "    elif type(obj) == scipy.io.matlab.mio5_params.mat_struct:\n",
    "        ret = {}\n",
    "        for field_name in obj._fieldnames:            \n",
    "            field = generate_dataset_obj(obj.__dict__[field_name])\n",
    "            if field_name in must_be_list_fields:\n",
    "                field = [field]\n",
    "                ret[field_name] = field\n",
    "\n",
    "    else:\n",
    "        ret = obj\n",
    "\n",
    "    return ret\n",
    "\n",
    "########################################################################################################################################################\n",
    "def print_dataset_obj(obj, depth = 0, maxIterInArray = 20):\n",
    "    prefix = \"  \"*depth\n",
    "    if type(obj) == dict:\n",
    "        for key in obj.keys():\n",
    "            print(\"{}{}\".format(prefix, key))\n",
    "            print_dataset_obj(obj[key], depth + 1)\n",
    "    elif type(obj) == list:\n",
    "        for i, value in enumerate(obj):\n",
    "            if i >= maxIterInArray:\n",
    "                break\n",
    "            print(\"{}{}\".format(prefix, i))\n",
    "            print_dataset_obj(value, depth + 1)\n",
    "    else:\n",
    "        print(\"{}{}\".format(prefix, obj))\n",
    "########################################################################################################################################################\n",
    "def return_image_joints(name,data):\n",
    "    for item in data: # guardar coordenadas de los joints\n",
    "        if item[0] == name:\n",
    "            #print(item[1]) \n",
    "            return item[1]\n",
    "########################################################################################################################################################\n",
    "rightconnections = [\n",
    "                    (0,1),(1,2),(3,4),(4,5),(2,6),\n",
    "                    (3,6),(6,7),(7,8),(8,9),(10,11),\n",
    "                    (11,12),(12,7),(13,7),(13,14),(14,15)\n",
    "                   ]\n",
    "size_img_x = 256\n",
    "size_img_y = 256\n",
    "def draw_img_joints(file_name, data, resize = False ):    \n",
    "    # Load image\n",
    "    #img = cv2.imread(Path_To_Single_Person_Images + \"/\" + file_name,1)  \n",
    "    img = image.load_img(Path_To_Single_Person_Images + \"/\" + file_name)\n",
    "    img = image.img_to_array(img) \n",
    "    img = img/255\n",
    "    if resize:\n",
    "        img = np.float32(tf.image.resize(img,(size_img_x, size_img_y)))  \n",
    "    pts = return_image_joints(file_name, data)        \n",
    "    #plt.imshow(img)  \n",
    "    X = [x[0] for x in pts]\n",
    "    Y = [y[1] for y in pts]\n",
    "    X = [int(x) for x in X]\n",
    "    Y = [int(y) for y in Y]\n",
    "    \n",
    "    for i in range(16):\n",
    "        for j in range(16):\n",
    "            if (i,j) in rightconnections:\n",
    "                if X[i]>0 and X[j]>0 and Y[i]>0 and Y[j]>0:\n",
    "                    img = cv2.line(img,(X[i],Y[i]),(X[j],Y[j]),(1,0,0),5)\n",
    "                    plt.scatter(X[i], Y[i], marker=\"o\", color=\"red\", s=20)\n",
    "                    plt.scatter(X[j], Y[j], marker=\"o\", color=\"red\", s=20)\n",
    "                    \n",
    "    plt.imshow(img)\n",
    "########################################################################################################################################################\n",
    "def load_image(train_data, a, b):\n",
    "    train = np.asarray(train_data[a:b])\n",
    "    train_image = np.zeros((b-a,size_img_x,size_img_y,3))\n",
    "    for i in tqdm(range(a,b)):\n",
    "        name_img = train[i][0]\n",
    "        img = image.load_img(Path_To_Single_Person_Images + '/' + name_img)\n",
    "        img = image.img_to_array(img)\n",
    "        img_x = img.shape[1]\n",
    "        img_y = img.shape[0]\n",
    "        scala_x = img_x / size_img_x\n",
    "        scala_y = img_y / size_img_y        \n",
    "        for j in range(len(train[i][1])): # escala los puntos clave            \n",
    "                train[i][1][j] = np.array([train[i][1][j][0] / scala_x, train[i][1][j][1] / scala_y])            \n",
    "        img = tf.image.resize(img,(size_img_x, size_img_y))        \n",
    "        img = img/255\n",
    "        train_image[i] = img\n",
    "    return train_image, train\n",
    "########################################################################################################################################################\n",
    "def MakeHeatmap(x, y, width, height, show = False):\n",
    "    # Probability as a function of distance from the center derived\n",
    "    # from a gaussian distribution with mean = 0 and stdv = 1\n",
    "    scaledGaussian = lambda x : exp(-(1/2)*(x**2))\n",
    "\n",
    "    imgSize = (height, width)\n",
    "    center_x = x\n",
    "    center_y = y\n",
    "\n",
    "    isotropicGrayscaleImage = np.zeros((imgSize[0],imgSize[1]),np.uint8)\n",
    "    \n",
    "    if center_x > 0 and center_y > 0 :\n",
    "        for i in range(imgSize[0]):\n",
    "            for j in range(imgSize[1]):\n",
    "\n",
    "                # find euclidian distance from center of image (x,y) \n",
    "                # and scale it to range of 0 to 2.5 as scaled Gaussian\n",
    "                # returns highest probability for x=0 and approximately\n",
    "                # zero probability for x > 2.5\n",
    "\n",
    "                distanceFromCenter = np.linalg.norm(np.array([i-center_y,j-center_x]))\n",
    "                #distanceFromCenter = 18*distanceFromCenter/(imgSize/2)\n",
    "                scaledGaussianProb = scaledGaussian(distanceFromCenter)\n",
    "                isotropicGrayscaleImage[i,j] = np.clip(scaledGaussianProb*255,0,255)   \n",
    "\n",
    "        return isotropicGrayscaleImage\n",
    "    else: \n",
    "        return isotropicGrayscaleImage\n",
    "########################################################################################################################################################    \n",
    "def Joints_heatmaps(lista_de_joints, heatmap_size_x, heatmap_size_y, num_heatmaps = 16, show = False):\n",
    "    heatmaps = np.zeros((16,64,64))\n",
    "    for i in range(num_heatmaps):\n",
    "        x, y = lista_de_joints[i] \n",
    "        x = x / 4 # entre 4 por que el array es de 256x256\n",
    "        y = y / 4 # entre 4 por que el array es de 256x256\n",
    "        heatmaps[i] = MakeHeatmap(x, y, heatmap_size_x, heatmap_size_y)\n",
    "    if show:\n",
    "        plotImages(heatmaps, num_heatmaps)\n",
    "    return heatmaps\n",
    "########################################################################################################################################################        \n",
    "def plotImages(images_arr, num_images):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "########################################################################################################################################################    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar datos, Generación de heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################################################\n",
    "#darle formato de diccionario\n",
    "matph = './mpii.mat'\n",
    "decoded1 = loadmat(matph, struct_as_record=False)[\"RELEASE\"]\n",
    "must_be_list_fields = [\"annolist\",\"image\",\"name\", \"annorect\", \"scale\", \"x\", \"y\", \"annopoints\", \"point\", \"id\"]\n",
    "# Convert to dict\n",
    "dataset_obj = generate_dataset_obj(decoded1)\n",
    "# Print it out\n",
    "#print_dataset_obj(dataset_obj)\n",
    "len(dataset_obj['annolist'][0])\n",
    "#solo queremos la información en 'annolist'\n",
    "dataset = dataset_obj['annolist'][0]\n",
    "########################################################################################################################################################\n",
    "#guardamos solo informacion de las imagenes que tienen solo una persona\n",
    "train_data = []\n",
    "for i in range(len(dataset)):\n",
    "    if \"annopoints\" in dataset[i]['annorect'][0]:     \n",
    "        name = dataset[i]['image'][0]['name'][0]\n",
    "        tupla = [] \n",
    "        for j in range(16):     #ordena los puntos de articulaciones del id = 0 al id = 15       \n",
    "            try:\n",
    "                x = dataset[i]['annorect'][0]['annopoints'][0]['point'][0][j]['x'][0]\n",
    "            except:\n",
    "                x = -1\n",
    "            try:\n",
    "                y = dataset[i]['annorect'][0]['annopoints'][0]['point'][0][j]['y'][0]\n",
    "            except:\n",
    "                y = -1\n",
    "            try:\n",
    "                id = dataset[i]['annorect'][0]['annopoints'][0]['point'][0][j]['id'][0]\n",
    "            except:\n",
    "                id = -1          \n",
    "            tupla.append((x,y,id))\n",
    "        tupla = sorted(tupla, key = itemgetter(2)) # esto lo ordena\n",
    "        for j in range(len(tupla)):   #quita id de las tuplas,\n",
    "            tupla[j] = tupla[j][:2]\n",
    "        #pasa de tupla a array\n",
    "        tupla = np.asarray(tupla)        \n",
    "        train_data.append((name,tupla))\n",
    "#Creamos un array para guardar los nombres        \n",
    "names = []\n",
    "for item in train_data:#guardar nombres de las imagenes que voy a usar en \"name\"\n",
    "    names.append(item[0])\n",
    "########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.    ,  -1.    ],\n",
       "       [ -1.    ,  -1.    ],\n",
       "       [ -1.    ,  -1.    ],\n",
       "       [ -1.    ,  -1.    ],\n",
       "       [ -1.    ,  -1.    ],\n",
       "       [ -1.    ,  -1.    ],\n",
       "       [ -1.    ,  -1.    ],\n",
       "       [ -1.    ,  -1.    ],\n",
       "       [ -1.    ,  -1.    ],\n",
       "       [533.    , 322.    ],\n",
       "       [515.0945, 277.1333],\n",
       "       [463.9055, 148.8667],\n",
       "       [353.    , 172.    ],\n",
       "       [426.    , 239.    ],\n",
       "       [513.    , 288.    ],\n",
       "       [552.    , 355.    ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 41.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "#Crear Carpeta para guardar imagenes del dataset\n",
    "Path_To_Raw_Images = 'DataSet/mpii_human_pose_v1_images'\n",
    "Path_To_Single_Person_Images = 'DataSet/mpii_human_pose_v1_images/SinglePersonImagesWithData'\n",
    "os.chdir(Path_To_Raw_Images)\n",
    "\n",
    "if os.path.isdir('SinglePersonImagesWithData') is False:\n",
    "    os.makedirs('SinglePersonImagesWithData')\n",
    "    for images in names:\n",
    "        shutil.move(images, 'SinglePersonImagesWithData')\n",
    "\n",
    "os.chdir('../../')\n",
    "\n",
    "#Demostración dibujar joints en imagenes con data\n",
    "#draw_img_joints('060111501.jpg',train_data)\n",
    "########################################################################################################################################################\n",
    "#Ahora cargamos las imagenes\n",
    "lista_de_imagenes, lista_de_joints = load_image(train_data,0,4)\n",
    "#np.save('lista_de_imagenes', lista_de_imagenes)\n",
    "########################################################################################################################################################\n",
    "#Ahora creamos los heatmaps para lista_de_imagenes:\n",
    "heatmap_size_x = 64\n",
    "heatmap_size_y = 64 \n",
    "#dibujo_test_heatmaps = Joints_heatmaps(lista_de_joints[0][1], heatmap_size_x, heatmap_size_y, show = False)\n",
    "#creamos los heatmaps de nuestra data\n",
    "lista_de_heatmaps = np.zeros((4,64,64,16))\n",
    "for i in tqdm(range(lista_de_joints.shape[0])):\n",
    "    joints = return_image_joints(lista_de_joints[i][0], lista_de_joints)\n",
    "    lista_de_heatmaps[i] = np.moveaxis(Joints_heatmaps(joints, heatmap_size_x, heatmap_size_y), 0, -1) # change shape from 16x64x64 to 64x64x16\n",
    "#guardamos el array    \n",
    "#np.save('lista_de_heatmaps', lista_de_heatmaps)\n",
    "#liberamos memoria cargando los archivos desde disco\n",
    "#lista_de_heatmaps = np.load('lista_de_heatmaps.npy', mmap_mode='r')  \n",
    "#lista_de_imagenes = np.load('lista_de_imagenes.npy', mmap_mode='r') \n",
    "#plotImages(lista_de_heatmaps[67], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAALOCAYAAAAk3XagAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAWEElEQVR4nOzasQ0CQQwAQf5Fa5RAlZTwvWEqAMEGnBAzqS9waK1um5kTAAAAAAB8al+9AAAAAAAAv0lgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIDm/Gl7263xrEQAAWO2437Z337qVAQD4J89uZT+YAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBEYAYAAAAAIBGYAQAAAABIBGYAAAAAABKBGQAAAACARGAGAAAAACARmAEAAAAASARmAAAAAAASgRkAAAAAgGSbmdU7AACPdu2YBAAAgGGYf9fz0G+QyCgFAACAQw5mAAAAAAASgRkAAAAAgERgBgAAAAAgEZgBAAAAAEgEZgAAAAAAEoEZAAAAAIBkwGMTlylYdR8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotImages(np.moveaxis(lista_de_heatmaps[3], -1 , 0 ), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['061185289.jpg',\n",
       "       array([[ -0.35555556,  -0.53333333],\n",
       "       [ -0.35555556,  -0.53333333],\n",
       "       [ -0.35555556,  -0.53333333],\n",
       "       [ -0.35555556,  -0.53333333],\n",
       "       [ -0.35555556,  -0.53333333],\n",
       "       [ -0.35555556,  -0.53333333],\n",
       "       [ -0.35555556,  -0.53333333],\n",
       "       [ -0.35555556,  -0.53333333],\n",
       "       [ -0.35555556,  -0.53333333],\n",
       "       [189.51111111, 171.73333333],\n",
       "       [183.14471111, 147.80442667],\n",
       "       [164.94417778,  79.39557333],\n",
       "       [125.51111111,  91.73333333],\n",
       "       [151.46666667, 127.46666667],\n",
       "       [182.4       , 153.6       ],\n",
       "       [196.26666667, 189.33333333]])], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_de_joints[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
